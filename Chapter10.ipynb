{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiUzjHfVsVQowbwivMG0NQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yu0ki/BERT_Practice/blob/main/Chapter10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "hLGHpQtB4EVb",
        "outputId": "625edf04-10cc-4f35-ece8-64cef258d53b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBERTは文章をトークン化したものを入力すると、各トークンに対応するベクトルを返してくれる\\nこのベクトルを集約することで、文章全体の意味を反映したベクトル(=文章ベクトル)を作ることができる\\n\\nそして、こうしたベクトルの類似度を求めることで文章同士の内容の類似度を比べられる\\n\\n\\n文章ベクトルのゲット方法 - 2パターン\\n1. [CLS]に対応するベクトルを使う\\n2. BERTからの出力ベクトルの平均を取る\\n\\nBERTの元論文では1が使われていた\\nしかし、最近の論文では2の方が性能が良いという結果も出ている\\n今回は2で行くことにする\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "'''\n",
        "BERTは文章をトークン化したものを入力すると、各トークンに対応するベクトルを返してくれる\n",
        "このベクトルを集約することで、文章全体の意味を反映したベクトル(=文章ベクトル)を作ることができる\n",
        "\n",
        "そして、こうしたベクトルの類似度を求めることで文章同士の内容の類似度を比べられる\n",
        "\n",
        "\n",
        "文章ベクトルのゲット方法 - 2パターン\n",
        "1. [CLS]に対応するベクトルを使う\n",
        "2. BERTからの出力ベクトルの平均を取る\n",
        "\n",
        "BERTの元論文では1が使われていた\n",
        "しかし、最近の論文では2の方が性能が良いという結果も出ている\n",
        "今回は2で行くことにする\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir chap10\n",
        "%cd ./chap10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ssjBQ2g4l6Q",
        "outputId": "d83b1745-5a79-4d21-d1b1-4b18b69a0055"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/chap10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ライブラリインストール\n",
        "!pip install transformers==4.18.0 fugashi==1.1.0 ipadic==1.0.0\n",
        "\n",
        "import random\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertJapaneseTokenizer, BertModel\n",
        "\n",
        "# BERTの日本語モデル\n",
        "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkolZzb84yTv",
        "outputId": "00881d25-cf53-4cc4-996f-017cab91ba4f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.18.0\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 28.0 MB/s \n",
            "\u001b[?25hCollecting fugashi==1.1.0\n",
            "  Downloading fugashi-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (486 kB)\n",
            "\u001b[K     |████████████████████████████████| 486 kB 71.3 MB/s \n",
            "\u001b[?25hCollecting ipadic==1.0.0\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.4 MB 57.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 61.6 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 65.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (4.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 59.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.18.0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.18.0) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0) (1.1.0)\n",
            "Building wheels for collected packages: ipadic, sacremoses\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556723 sha256=76fd17b71279acd2d43c926096a5abad0cd3de8488b20a6d3b1d3cef034113c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/8b/99/cf0d27191876637cd3639a560f93aa982d7855ce826c94348b\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=814c4ca996d3813526d45f771b2bfc7ad98d62e28695281fe4e9b367ff735a83\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built ipadic sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers, ipadic, fugashi\n",
            "Successfully installed fugashi-1.1.0 huggingface-hub-0.9.1 ipadic-1.0.0 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# とりあえずデータセットをダウンロードしよう\n",
        "# livedoorニュースコーパスを使うぞ\n",
        "\n",
        "#データのダウンロード\n",
        "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz \n",
        "#ファイルの解凍\n",
        "!tar -zxf ldcc-20140209.tar.gz "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaMwv0eQ5HMD",
        "outputId": "b8a52d9f-b263-49d9-cce9-7795b58f188d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-21 13:04:13--  https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
            "Resolving www.rondhuit.com (www.rondhuit.com)... 59.106.19.174\n",
            "Connecting to www.rondhuit.com (www.rondhuit.com)|59.106.19.174|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8855190 (8.4M) [application/x-gzip]\n",
            "Saving to: ‘ldcc-20140209.tar.gz’\n",
            "\n",
            "ldcc-20140209.tar.g 100%[===================>]   8.44M  3.03MB/s    in 2.8s    \n",
            "\n",
            "2022-09-21 13:04:17 (3.03 MB/s) - ‘ldcc-20140209.tar.gz’ saved [8855190/8855190]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "データを取り込んで、文章ベクトル(1記事に対して1ベクトル)を出力しよう\n",
        "\n",
        "・　コーパス内のカテゴリーのリスト定義\n",
        "    ここに書いてある9個のカテゴリにニュースが分類されている\n",
        "    https://www.rondhuit.com/download.html#ldcc\n",
        "・　トークナイザ・モデルのロード\n",
        "\n",
        "・　文章を取り出してはトークン化・BERTに入れて、出力の平均を取る\n",
        "\n",
        "livedoorコーパスのデータ形式\n",
        "\n",
        "フォルダの位置\n",
        "./text/カテゴリ名/カテゴリ名-記事ID.txt　\n",
        "\n",
        "例えば\n",
        "./text/it-life-hack/it-life-hack-6342280.txt　\n",
        "\n",
        "txtファイルの中身\n",
        "\n",
        "# 一行目：URL\n",
        "# 二行目：作成日時\n",
        "# 三行目：記事タイトル\n",
        "# 四行目以降：本文\n",
        "という構成になっているはず\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "wUL9Bk156nTF",
        "outputId": "31307939-6e98-4d8e-fe5d-fa5de46e4789"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nデータを取り込んで、文章ベクトル(1記事に対して1ベクトル)を出力しよう\\n\\n・\\u3000コーパス内のカテゴリーのリスト定義\\n    ここに書いてある9個のカテゴリにニュースが分類されている\\n    https://www.rondhuit.com/download.html#ldcc\\n・\\u3000トークナイザ・モデルのロード\\n\\n・\\u3000文章を取り出してはトークン化・BERTに入れて、出力の平均を取る\\n\\nlivedoorコーパスのデータ形式\\n\\nフォルダの位置\\n./text/カテゴリ名/カテゴリ名-記事ID.txt\\u3000\\n\\n例えば\\n./text/it-life-hack/it-life-hack-6342280.txt\\u3000\\n\\ntxtファイルの中身\\n\\n# 一行目：URL\\n# 二行目：作成日時\\n# 三行目：記事タイトル\\n# 四行目以降：本文\\nという構成になっているはず\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# カテゴリリスト定義\n",
        "# コーパス中のデータのカテゴリ名一覧\n",
        "category_list = [\n",
        "    'dokujo-tsushin',\n",
        "    'it-life-hack',\n",
        "    'kaden-channel',\n",
        "    'livedoor-homme',\n",
        "    'movie-enter',\n",
        "    'peachy',\n",
        "    'smax',\n",
        "    'sports-watch',\n",
        "    'topic-news'\n",
        "]\n",
        "\n",
        "# トークナイザとモデルをロード\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = BertModel.from_pretrained(MODEL_NAME)\n",
        "model = model.cuda()\n",
        "\n",
        "\n",
        "\n",
        "# 文章を1記事分ずつ取り出し、文章を全部連結して符号化\n",
        "# それをBERTに突っ込んで出力ベクトルを得る\n",
        "# トークンごとのベクトルの平均を取る\n",
        "\n",
        "# 文章のトークンの最大系列長\n",
        "max_length = 256\n",
        "\n",
        "# 文章ベクトル\n",
        "sentence_vectors = []\n",
        "labels = [] \n",
        "\n",
        "for label, category in enumerate(category_list):\n",
        "    # まずはどのファイルの文章を読み込むか選ぶ\n",
        "    for file in glob.glob(f'./text/{category}/{category}*'):\n",
        "        # 行ごとに区切ってファイル内の文章を書き出す\n",
        "        lines = open(file).read().splitlines()\n",
        "\n",
        "        # 用事があるのは4行目以降なので、それより前は捨てる\n",
        "        # ４行目以降は、全ての文章を連結する\n",
        "        text = '\\n'.join(lines[3:])\n",
        "\n",
        "        # 符号化\n",
        "        encoding = tokenizer(\n",
        "            text,\n",
        "            max_length = max_length,\n",
        "            padding = 'max_length',\n",
        "            truncation = True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        encoding = { k : v.cuda() for k, v in encoding.items() }\n",
        "        # [PAD]に対応する部分が0他は1\n",
        "        attention_mask = encoding['attention_mask']\n",
        "\n",
        "        # BERTに突っ込みましょう\n",
        "        with torch.no_grad():\n",
        "            output = model(**encoding)\n",
        "\n",
        "            # 最終層の出力ベクトル\n",
        "            # BertModel の出力一覧：https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n",
        "            # logitsが使えるのはBertForSequenceClassification　：　https://huggingface.co/docs/transformers/model_doc/bert\n",
        "            last_hidden_state = output.last_hidden_state\n",
        "\n",
        "            # [PAD]以外のlast_hidden_stateの出力を平均する\n",
        "            # unsqueeze : tensorの次元を増やす（先頭に新たな次元を挿入）. -1を指定した場合は、末尾に新しい次元を挿入\n",
        "            # https://lilaboc.work/archives/23948835.html\n",
        "            # sum(1) : https://publicjournal.hatenablog.com/entry/2017/03/20/212838\n",
        "            average_hidden_state = (last_hidden_state * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True)\n",
        "            # print(f'({last_hidden_state.size()} * {attention_mask.unsqueeze(-1).size()}).sum(1) / {attention_mask.size()}.sum(1, keepdim=True)')\n",
        "            sentence_vectors.append(average_hidden_state[0].cpu().numpy())\n",
        "            labels.append(label)\n",
        "        \n",
        "\n",
        "# それぞれをnumpy.ndarrayにする。\n",
        "sentence_vectors = np.vstack(sentence_vectors)\n",
        "labels = np.array(labels)\n",
        "\n",
        "\n",
        "            "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOQ3Voyp8w2f",
        "outputId": "7ea12aa1-9b0e-4ada-f8e6-4376aa7df02b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    }
  ]
}