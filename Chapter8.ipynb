{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP65M0u4jzEodPCBELxlX7B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yu0ki/BERT_Practice/blob/main/Chapter8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "EqLjWGBsw_C9",
        "outputId": "54b19469-9eac-431c-d3a8-5c408be08efd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n本章でやること：固有表現抽出\\nつまり文章から人名、組織名といったものを抽出する\\nまずは簡単な方法を学んで、次にBERTを使ったものを学んでいく\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "'''\n",
        "本章でやること：固有表現抽出\n",
        "つまり文章から人名、組織名といったものを抽出する\n",
        "まずは簡単な方法を学んで、次にBERTを使ったものを学んでいく\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# まずはディレクトリ移動\n",
        "!mkdir chap8\n",
        "%cd ./chap8\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulWkNLj1xuDT",
        "outputId": "fc68020b-a7cf-42e6-9e14-c97ba9baff57"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/chap8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 次はライブラリ\n",
        "# バージョンが教科書のだと古いようなので、模範解答（https://github.com/stockmarkteam/bert-book/blob/master/Chapter6.ipynb）にバージョンを合わせている\n",
        "!pip install transformers==4.18.0 fugashi==1.1.0 ipadic==1.0.0 pytorch-lightning==1.6.1\n",
        "\n",
        "# 集合に関する数学的な操作のライブラリかな？\n",
        "# https://qiita.com/anmint/items/37ca0ded5e1d360b51f3\n",
        "import itertools\n",
        "\n",
        "import random\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# unicode文字列に対していろいろ操作できそうなライブラリ\n",
        "# https://docs.python.org/ja/3/library/unicodedata.html\n",
        "import unicodedata\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertJapaneseTokenizer, BertForTokenClassification\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# 日本語学習済みモデル\n",
        "MODEL_NAME = 'cl-tohoku/best-base-japanese-whole-word-masking'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VKOABi3yF89",
        "outputId": "5dd48f4f-d8c0-445d-db28-e93f26cb1538"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.18.0\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 6.7 MB/s \n",
            "\u001b[?25hCollecting fugashi==1.1.0\n",
            "  Downloading fugashi-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (486 kB)\n",
            "\u001b[K     |████████████████████████████████| 486 kB 45.0 MB/s \n",
            "\u001b[?25hCollecting ipadic==1.0.0\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.4 MB 47.0 MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning==1.6.1\n",
            "  Downloading pytorch_lightning-1.6.1-py3-none-any.whl (582 kB)\n",
            "\u001b[K     |████████████████████████████████| 582 kB 45.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 29.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (4.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (3.8.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 65.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (6.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 50.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.1) (2.8.0)\n",
            "Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.1) (1.12.1+cu113)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.1) (2022.8.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.1) (4.1.1)\n",
            "Collecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 62.4 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate<0.4.0,>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (3.8.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (22.1.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (1.8.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (6.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.18.0) (3.0.9)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.2.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.47.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (3.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.18.0) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (3.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0) (1.1.0)\n",
            "Building wheels for collected packages: ipadic, sacremoses\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556723 sha256=8b9cf035715c54b2a9d6498533d41a5d039b8bcf5dafb57149c904107e43cf81\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/8b/99/cf0d27191876637cd3639a560f93aa982d7855ce826c94348b\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=c24c69c8453d894f80b84a8bebf3b8e3a24caa00eab91f4b858b47f350156d70\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built ipadic sacremoses\n",
            "Installing collected packages: torchmetrics, tokenizers, sacremoses, pyDeprecate, huggingface-hub, transformers, pytorch-lightning, ipadic, fugashi\n",
            "Successfully installed fugashi-1.1.0 huggingface-hub-0.9.1 ipadic-1.0.0 pyDeprecate-0.3.2 pytorch-lightning-1.6.1 sacremoses-0.0.53 tokenizers-0.12.1 torchmetrics-0.9.3 transformers-4.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "固有表現抽出とは\n",
        "\n",
        "人名・組織名などの固有名詞\n",
        "日付などの時間表現\n",
        "金額などの数値表現\n",
        "\n",
        "などを抽出する。そして、それが人名なのかなんなのか、カテゴリを判定する\n",
        "\n",
        "ちなみに、どんなカテゴリが定義されているかは何パターンかあり、問題によって変える\n",
        "IREX\n",
        "拡張固有表現階層\n",
        "などがある\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "lkeYk-Xszf2e",
        "outputId": "693e5eaa-1525-4e06-b414-6c64d89043d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n固有表現抽出とは\\n\\n人名・組織名などの固有名詞\\n日付などの時間表現\\n金額などの数値表現\\n\\nなどを抽出する。そして、それが人名なのかなんなのか、カテゴリを判定する\\n\\nちなみに、どんなカテゴリが定義されているかは何パターンかあり、問題によって変える\\nIREX\\n拡張固有表現階層\\nなどがある\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "表記揺れについて\n",
        "\n",
        "固有表現には全角・半角などの違いで、表記揺れが発生する\n",
        "よって、これらを正規化することで１つに統一する必要がある\n",
        "\n",
        "正規化には、unicodedata.normalize('NFKC', text) が用いられる\n",
        "NFKCは正規化のモード\n",
        "textは正規化対象の文字列\n",
        "\n",
        "'''\n",
        "# 匿名関数lambda\n",
        "normalize = lambda s: unicodedata.normalize(\"NFKC\", s)\n",
        "\n",
        "# 全角ABCを半角に正規化\n",
        "print(f'ＡＢＣ　-> {normalize(\"ＡＢＣ\")}')\n",
        "# 半角ABCを半角に正規化\n",
        "print(f'ABC　-> {normalize(\"ABC\")}')\n",
        "\n",
        "# 全角１２３を半角に正規化\n",
        "print(f'１２３　-> {normalize(\"１２３\")}')\n",
        "# 半角ABCを半角に正規化\n",
        "print(f'123　-> {normalize(\"123\")}')\n",
        "\n",
        "# 全角カタカナを全角に正規化\n",
        "print(f'アイウ　-> {normalize(\"アイウ\")}')\n",
        "# 半角カタカナを全角に正規化\n",
        "print(f'ｱｲｳ　-> {normalize(\"ｱｲｳ\")}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cTRA9S40vL6",
        "outputId": "b995dfcd-7394-4dc7-a880-e78de8b2f48f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ＡＢＣ　-> ABC\n",
            "ABC　-> ABC\n",
            "１２３　-> 123\n",
            "123　-> 123\n",
            "アイウ　-> アイウ\n",
            "ｱｲｳ　-> アイウ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "固有表現抽出方法１\n",
        "IO法\n",
        "\n",
        "簡単なアルゴリズムであるが、一般的にはBIO法の方がよく使われる\n",
        "    IO法でもまあまあいいが、一部対応できない表現があるためBIOが使われる傾向にある\n",
        "\n",
        "アルゴリズム\n",
        "１。トークンが固有表現の一部であれば、トークンにタグ「I-(Type)」をつける.  ex) I - (人名)　など\n",
        "２。トークンが固有表現の一部でないならば、トークンのタグはOとする\n",
        "3。O以外の同じタグが連続している部分トークン列を連結して、固有表現とする\n",
        "    トークナイザがトークン分割の際に「##」を付与している場合があるので、これは除外して結合する\n",
        "\n",
        "I は Inside, O は Outsideから来ている\n",
        "\n",
        "\n",
        "問題点\n",
        "同じタグを持つ固有名詞が複数続いた場合は、全てが連結されて一つの固有表現として抽出される\n",
        "日米　などは日本とアメリカで別々のはずなのに、1個に合体してしまう\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "5cuuzIi86pMq",
        "outputId": "20951f5f-9616-4f8e-a9c6-a4e48a4b0af2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n固有表現抽出方法１\\nIO法\\n\\n簡単なアルゴリズムであるが、一般的にはBIO法の方がよく使われる\\n    IO法でもまあまあいいが、一部対応できない表現があるためBIOが使われる傾向にある\\n\\nアルゴリズム\\n１。トークンが固有表現の一部であれば、トークンにタグ「I-(Type)」をつける.  ex) I - (人名)\\u3000など\\n２。トークンが固有表現の一部でないならば、トークンのタグはOとする\\n3。O以外の同じタグが連続している部分トークン列を連結して、固有表現とする\\n    トークナイザがトークン分割の際に「##」を付与している場合があるので、これは除外して結合する\\n\\nI は Inside, O は Outsideから来ている\\n\\n\\n問題点\\n同じタグを持つ固有名詞が複数続いた場合は、全てが連結されて一つの固有表現として抽出される\\n日米\\u3000などは日本とアメリカで別々のはずなのに、1個に合体してしまう\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 固有表現では普段のトークンにタグを付与するので\n",
        "# それに対応したトークナイザを定義しないといけない\n",
        "\n",
        "# そのトークナイザの中では、学習・推論に使用する関数を定義\n",
        "# ここは最悪あんまり理解してなくてもいいらしい\n",
        "\n",
        "class NER_tokenizer(BertJapaneseTokenizer):\n",
        "\n",
        "    '''\n",
        "    文章と、それに含まれる固有表現が与えられたときに、符号化とラベル列の作成を行う関数\n",
        "    この関数の出力は、BERTモデルに入力できる形になっている。（学習）\n",
        "    つまり、固有表現に適切なラベルを付与してBERTに入力できるように変換する関数\n",
        "    これをBERTに入力すると、ラベルがついているため、損失を返してくれる\n",
        "    これをもとにパラメータを学習する\n",
        "    '''\n",
        "    def encode_plus_tagged(self, text, entities, max_length):\n",
        "\n",
        "        # 手順１：固有表現の前後でtextを分割し、それぞれのラベルをつけておく\n",
        "\n",
        "        # entities は配列\n",
        "        # 配列のようそはdict型\n",
        "        # {'name' : 固有表現, 'span': [文章中の文字列の開始位置index, 終了index+1], 'type' : 固有表現のラベル, 'type_id' : typeに紐づく数字（１：１対応）}\n",
        "        # sorted :https://note.nkmk.me/python-list-sort-sorted/\n",
        "        # 単語の文章中での開始位置でソート\n",
        "        entities = sorted(entities, key=lambda x: x['span'][0])\n",
        "\n",
        "        # 分割後の文字列を追加する\n",
        "        splitted = []\n",
        "\n",
        "        # 次にラベルをつけるのはどこからか？を表す\n",
        "        position = 0\n",
        "\n",
        "        for entity in entities:\n",
        "            start = entitiy['span'][0]\n",
        "            end = entity['span'][1]\n",
        "\n",
        "            # 固有表現のラベル\n",
        "            label = entity['type_id']\n",
        "\n",
        "            # 固有表現でないものにはラベルOをつける\n",
        "            splitted.append({'text' : text[position:start], 'label' : 0})\n",
        "\n",
        "            # 固有表現には対応するラベルをIDで付与\n",
        "            splitted.append({'text' : text[start:end], 'label' : label})\n",
        "\n",
        "            # 次にラベルをつけるのはend以降\n",
        "            position = end\n",
        "\n",
        "        # 最後余った文字列をラベル0にする\n",
        "        splitted.append({'text' : text[position:], 'label' : 0})\n",
        "\n",
        "        # 長さ０の文字列をsplittedから取り除く\n",
        "        splitted = { s for s in splitted if s['text']}\n",
        "\n",
        "        # 手順２：分割されたそれぞれの文字列をトークン化し、ラベルづけする\n",
        "\n",
        "        # トークンを入れる\n",
        "        tokens = []\n",
        "        # ラベルを入れる\n",
        "        labels = []\n",
        "\n",
        "        for text_splitted in splitted:\n",
        "            text = text_splitted['text']\n",
        "            label = text_splitted['label']\n",
        "\n",
        "            # テキストをトークン化\n",
        "            tokens_splitted = self.tokenizer(text)\n",
        "            # 対応するラベルを追加\n",
        "            labels_splitted = [label] * len(tokens_splitted)\n",
        "\n",
        "            tokens.extend(tokens_splitted)\n",
        "            labels.extend(labels_splitted)\n",
        "\n",
        "        \n",
        "        # 手順３：符号化を行い、BERTに入力可能にする\n",
        "        input_ids = self.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # 公式ドキュメントが見つからないが\n",
        "        # prepare_for_model はinput_idsを符号化するらしい\n",
        "        # 公式これか？？https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.prepare_for_model\n",
        "\n",
        "        encoding = self.prepare_for_model(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "\n",
        "        # 特殊トークン[CLS], [SEP]のラベルを０にする\n",
        "        labels = [0] + labels[:max_length-2] + [0]\n",
        "\n",
        "        # [PAD]もラベルを０に\n",
        "        # [PAD]は文末にあるので、その数だけ最後に追加すればいい\n",
        "        labels = labels + [0]*(max_length - len(labels))\n",
        "\n",
        "        encoding['labels'] = labels\n",
        "\n",
        "        return encoding\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wQepsOxT-3Ce"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}