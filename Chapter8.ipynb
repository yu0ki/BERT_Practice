{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPiCy47wB/16/23igULeQih",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yu0ki/BERT_Practice/blob/main/Chapter8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EqLjWGBsw_C9",
        "outputId": "c40d5c48-8da7-47e8-d503-92fe6705bff0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n本章でやること：固有表現抽出\\nつまり文章から人名、組織名といったものを抽出する\\nまずは簡単な方法を学んで、次にBERTを使ったものを学んでいく\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "'''\n",
        "本章でやること：固有表現抽出\n",
        "つまり文章から人名、組織名といったものを抽出する\n",
        "まずは簡単な方法を学んで、次にBERTを使ったものを学んでいく\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# まずはディレクトリ移動\n",
        "!mkdir chap8\n",
        "%cd ./chap8\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulWkNLj1xuDT",
        "outputId": "d8b4f60b-c1b1-4b15-ab00-bc1051f28d9a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘chap8’: File exists\n",
            "/content/chap8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 次はライブラリ\n",
        "# バージョンが教科書のだと古いようなので、模範解答（https://github.com/stockmarkteam/bert-book/blob/master/Chapter6.ipynb）にバージョンを合わせている\n",
        "!pip install transformers==4.18.0 fugashi==1.1.0 ipadic==1.0.0 pytorch-lightning==1.6.1\n",
        "\n",
        "# 集合に関する数学的な操作のライブラリかな？\n",
        "# https://qiita.com/anmint/items/37ca0ded5e1d360b51f3\n",
        "import itertools\n",
        "\n",
        "import random\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# unicode文字列に対していろいろ操作できそうなライブラリ\n",
        "# https://docs.python.org/ja/3/library/unicodedata.html\n",
        "import unicodedata\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertJapaneseTokenizer, BertForTokenClassification\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# 日本語学習済みモデル\n",
        "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VKOABi3yF89",
        "outputId": "43f80ce2-ab2c-48c5-9518-b7c91a36c40c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==4.18.0 in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: fugashi==1.1.0 in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: ipadic==1.0.0 in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: pytorch-lightning==1.6.1 in /usr/local/lib/python3.7/dist-packages (1.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (0.9.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (0.0.53)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (4.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.1) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.1) (4.1.1)\n",
            "Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.1) (0.9.3)\n",
            "Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.1) (0.3.2)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.1) (2.8.0)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.1) (2022.8.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (3.8.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (1.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (22.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (6.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1) (1.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.18.0) (3.0.9)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.47.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.2.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (4.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.18.0) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1) (3.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "固有表現抽出とは\n",
        "\n",
        "人名・組織名などの固有名詞\n",
        "日付などの時間表現\n",
        "金額などの数値表現\n",
        "\n",
        "などを抽出する。そして、それが人名なのかなんなのか、カテゴリを判定する\n",
        "\n",
        "ちなみに、どんなカテゴリが定義されているかは何パターンかあり、問題によって変える\n",
        "IREX\n",
        "拡張固有表現階層\n",
        "などがある\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "lkeYk-Xszf2e",
        "outputId": "a7fde109-6ffa-4303-a84b-ac5f3176122c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n固有表現抽出とは\\n\\n人名・組織名などの固有名詞\\n日付などの時間表現\\n金額などの数値表現\\n\\nなどを抽出する。そして、それが人名なのかなんなのか、カテゴリを判定する\\n\\nちなみに、どんなカテゴリが定義されているかは何パターンかあり、問題によって変える\\nIREX\\n拡張固有表現階層\\nなどがある\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "表記揺れについて\n",
        "\n",
        "固有表現には全角・半角などの違いで、表記揺れが発生する\n",
        "よって、これらを正規化することで１つに統一する必要がある\n",
        "\n",
        "正規化には、unicodedata.normalize('NFKC', text) が用いられる\n",
        "NFKCは正規化のモード\n",
        "textは正規化対象の文字列\n",
        "\n",
        "'''\n",
        "# 匿名関数lambda\n",
        "normalize = lambda s: unicodedata.normalize(\"NFKC\", s)\n",
        "\n",
        "# 全角ABCを半角に正規化\n",
        "print(f'ＡＢＣ　-> {normalize(\"ＡＢＣ\")}')\n",
        "# 半角ABCを半角に正規化\n",
        "print(f'ABC　-> {normalize(\"ABC\")}')\n",
        "\n",
        "# 全角１２３を半角に正規化\n",
        "print(f'１２３　-> {normalize(\"１２３\")}')\n",
        "# 半角ABCを半角に正規化\n",
        "print(f'123　-> {normalize(\"123\")}')\n",
        "\n",
        "# 全角カタカナを全角に正規化\n",
        "print(f'アイウ　-> {normalize(\"アイウ\")}')\n",
        "# 半角カタカナを全角に正規化\n",
        "print(f'ｱｲｳ　-> {normalize(\"ｱｲｳ\")}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cTRA9S40vL6",
        "outputId": "5a513ab2-8106-43e8-8ead-95bed7ac8b3b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ＡＢＣ　-> ABC\n",
            "ABC　-> ABC\n",
            "１２３　-> 123\n",
            "123　-> 123\n",
            "アイウ　-> アイウ\n",
            "ｱｲｳ　-> アイウ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "固有表現抽出方法１\n",
        "IO法\n",
        "\n",
        "簡単なアルゴリズムであるが、一般的にはBIO法の方がよく使われる\n",
        "    IO法でもまあまあいいが、一部対応できない表現があるためBIOが使われる傾向にある\n",
        "\n",
        "アルゴリズム\n",
        "１。トークンが固有表現の一部であれば、トークンにタグ「I-(Type)」をつける.  ex) I - (人名)　など\n",
        "２。トークンが固有表現の一部でないならば、トークンのタグはOとする\n",
        "3。O以外の同じタグが連続している部分トークン列を連結して、固有表現とする\n",
        "    トークナイザがトークン分割の際に「##」を付与している場合があるので、これは除外して結合する\n",
        "\n",
        "I は Inside, O は Outsideから来ている\n",
        "\n",
        "\n",
        "問題点\n",
        "同じタグを持つ固有名詞が複数続いた場合は、全てが連結されて一つの固有表現として抽出される\n",
        "日米　などは日本とアメリカで別々のはずなのに、1個に合体してしまう\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "5cuuzIi86pMq",
        "outputId": "d49d7e29-420a-4a1f-c333-e527440337a5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n固有表現抽出方法１\\nIO法\\n\\n簡単なアルゴリズムであるが、一般的にはBIO法の方がよく使われる\\n    IO法でもまあまあいいが、一部対応できない表現があるためBIOが使われる傾向にある\\n\\nアルゴリズム\\n１。トークンが固有表現の一部であれば、トークンにタグ「I-(Type)」をつける.  ex) I - (人名)\\u3000など\\n２。トークンが固有表現の一部でないならば、トークンのタグはOとする\\n3。O以外の同じタグが連続している部分トークン列を連結して、固有表現とする\\n    トークナイザがトークン分割の際に「##」を付与している場合があるので、これは除外して結合する\\n\\nI は Inside, O は Outsideから来ている\\n\\n\\n問題点\\n同じタグを持つ固有名詞が複数続いた場合は、全てが連結されて一つの固有表現として抽出される\\n日米\\u3000などは日本とアメリカで別々のはずなのに、1個に合体してしまう\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 固有表現では普段のトークンにタグを付与するので\n",
        "# それに対応したトークナイザを定義しないといけない\n",
        "\n",
        "# そのトークナイザの中では、学習・推論に使用する関数を定義\n",
        "# ここは最悪あんまり理解してなくてもいいらしい\n",
        "\n",
        "\n",
        "# BertJapaneseTokenizerを継承：つまり、selfはBertJapaneseTokenizerの関数は使える\n",
        "class NER_tokenizer(BertJapaneseTokenizer):\n",
        "\n",
        "    '''\n",
        "    文章と、それに含まれる固有表現が与えられたときに、符号化とラベル列の作成を行う関数\n",
        "    この関数の出力は、BERTモデルに入力できる形になっている。（学習時に使用する関数）\n",
        "    つまり、固有表現に適切なラベルを付与してBERTに入力できるように変換する関数\n",
        "    これをBERTに入力すると、ラベルがついているため、損失を返してくれる\n",
        "    これをもとにパラメータを学習する\n",
        "    '''\n",
        "    def encode_plus_tagged(self, text, entities, max_length):\n",
        "\n",
        "        # 手順１：固有表現の前後でtextを分割し、それぞれのラベルをつけておく\n",
        "\n",
        "        # entities は配列\n",
        "        # 配列のようそはdict型\n",
        "        # {'name' : 固有表現, 'span': [文章中の文字列の開始位置index, 終了index+1], 'type' : 固有表現のラベル, 'type_id' : typeに紐づく数字（１：１対応）}\n",
        "        # sorted :https://note.nkmk.me/python-list-sort-sorted/\n",
        "        # 単語の文章中での開始位置でソート\n",
        "        entities = sorted(entities, key=lambda x: x['span'][0])\n",
        "\n",
        "        # 分割後の文字列を追加する\n",
        "        splitted = []\n",
        "\n",
        "        # 次にラベルをつけるのはどこからか？を表す\n",
        "        position = 0\n",
        "\n",
        "        '''\n",
        "        entitiesの中には、文章中に入っている固有表現とそのラベル一覧が入っている（教師データ）\n",
        "        textを普段通り分割すると、固有表現が途中で分断されてしまって、教師データと一致する固有表現が検出できなくなってしまうことがある\n",
        "        これを回避するために、以下のfor文では教師データの固有表現を見つけては、textを分割するという対応をとっている\n",
        "        分割ついでにラベルもちゃんと記録する\n",
        "        '''\n",
        "        for entity in entities:\n",
        "            start = entity['span'][0]\n",
        "            end = entity['span'][1]\n",
        "\n",
        "            # 固有表現のラベル\n",
        "            label = entity['type_id']\n",
        "\n",
        "            # 固有表現でないものにはラベルOをつける\n",
        "            splitted.append({'text' : text[position:start], 'label' : 0})\n",
        "\n",
        "            # 固有表現には対応するラベルをIDで付与\n",
        "            splitted.append({'text' : text[start:end], 'label' : label})\n",
        "\n",
        "            # 次にラベルをつけるのはend以降\n",
        "            position = end\n",
        "\n",
        "        # 最後余った文字列をラベル0にする\n",
        "        splitted.append({'text' : text[position:], 'label' : 0})\n",
        "\n",
        "        # 長さ０の文字列をsplittedから取り除く\n",
        "        splitted = [ s for s in splitted if s['text']]\n",
        "\n",
        "        # 手順２：分割されたそれぞれの文字列をトークン化し、ラベルづけする\n",
        "\n",
        "        # トークンを入れる\n",
        "        tokens = []\n",
        "        # ラベルを入れる\n",
        "        labels = []\n",
        "\n",
        "        for text_splitted in splitted:\n",
        "            text = text_splitted['text']\n",
        "            label = text_splitted['label']\n",
        "\n",
        "            # テキストをトークン化\n",
        "            tokens_splitted = self.tokenize(text)\n",
        "            # 対応するラベルを追加\n",
        "            labels_splitted = [label] * len(tokens_splitted)\n",
        "\n",
        "            tokens.extend(tokens_splitted)\n",
        "            labels.extend(labels_splitted)\n",
        "\n",
        "        \n",
        "        # 手順３：符号化を行い、BERTに入力可能にする\n",
        "        input_ids = self.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # 公式ドキュメントが見つからないが\n",
        "        # prepare_for_model はinput_idsを符号化するらしい\n",
        "        # 公式これか？？https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.prepare_for_model\n",
        "\n",
        "        encoding = self.prepare_for_model(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "\n",
        "        # 特殊トークン[CLS], [SEP]のラベルを０にする\n",
        "        labels = [0] + labels[:max_length-2] + [0]\n",
        "\n",
        "        # [PAD]もラベルを０に\n",
        "        # [PAD]は文末にあるので、その数だけ最後に追加すればいい\n",
        "        labels = labels + [0]*(max_length - len(labels))\n",
        "\n",
        "        encoding['labels'] = labels\n",
        "\n",
        "        return encoding\n",
        "\n",
        "    \n",
        "    '''\n",
        "    文章を符号化するとともに、各トークンの文章中の位置を特定する関数\n",
        "    文章中に未知語などが含まれていた場合、先ほどの関数でラベルづけした出力（input_ids やそれに対応するlabel 元の文章は保存されない）からは\n",
        "    元の文章を復元して、固有表現はこれだ！と特定できない。\n",
        "    そのため、位置を特定しておく必要がある\n",
        "    （推論時に使用する関数）\n",
        "    '''\n",
        "    def encode_plus_untagged(\n",
        "        self, text, max_length=None, return_tensors=None\n",
        "    ):\n",
        "        # 手順１：文章のトークン化\n",
        "\n",
        "        # トークンを追加していく\n",
        "        tokens = []\n",
        "        # トークンに対応する文章中の文字列を追加していく\n",
        "        tokens_original = []\n",
        "\n",
        "        # MeCabで単語に分割\n",
        "        # https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/bert_japanese/tokenization_bert_japanese.html\n",
        "        # self.word_tokenizerにはBasicTokenizerというクラスのインスタンスが入ってるっぽい\n",
        "        words = self.word_tokenizer.tokenize(text)\n",
        "        # print('1')\n",
        "        # 手順２：単語をサブワード分割して、元のテキストを保存\n",
        "        for word in words:\n",
        "            # 単語をサブワードに分割\n",
        "            # 未知語も分割すれば対応可能になる可能性が上がるっぽいね　　　例）東京タワーが未知語でも、東京とタワーはそれぞれ既知の単語かもしれない\n",
        "            # https://note.com/npaka/n/nb08941a36c8b#I3MMH\n",
        "            tokens_word = self.subword_tokenizer.tokenize(word)\n",
        "            tokens.extend(tokens_word)\n",
        "\n",
        "            # 未知語への対応\n",
        "            # 未知語に遭遇した場合は元のテキストを残しておかないと、全て[UNK]に変換されてしまって区別できなくなる\n",
        "            if tokens_word[0] == '[UNK]':\n",
        "                tokens_original.append(word)\n",
        "            else:\n",
        "                # 先頭の単語以外は##を除去して保存\n",
        "                # 未知語対策のifを挟まないと、[UNK]が直で保存される\n",
        "                tokens_original.extend([\n",
        "                    token.replace('##', '') for token in tokens_word\n",
        "                ])\n",
        "        # print('2')\n",
        "        # 手順３：各トークンの文章中での位置を保存（空白位置も考慮）\n",
        "        position = 0\n",
        "        spans = [] # トークンの位置情報を追加していく\n",
        "\n",
        "        for token in tokens_original:\n",
        "            # トークン長\n",
        "            l = len(token)\n",
        "\n",
        "            # text（元の入力文章）中の[position..position+l]を切り出して、tokenと一致するか確認\n",
        "            # while 1 = while true\n",
        "            while 1:\n",
        "                # 一致しない場合\n",
        "                if token != text[position:position+l]:\n",
        "                    position += 1\n",
        "                else:\n",
        "                    spans.append([position, position+l])\n",
        "                    position += 1\n",
        "                    break\n",
        "\n",
        "        # print('3')\n",
        "        # 手順４：符号化を行い、BERT入力可能にする\n",
        "        input_ids = self.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # input_idsが既にわかっている場合に使用できる符号化\n",
        "        # 確かめてはいないが、いつもはBertJapanezeTokenizerのインスタンスを関数として呼び出していたが、それができないため、わざわざこの関数を使用しているのではないのか？と思っている\n",
        "        # selfに直接引数与えれるのかな・・・\n",
        "        encoding = self.prepare_for_model(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            padding='max_length' if max_length else False,\n",
        "            truncation = True if max_length else False\n",
        "        )\n",
        "\n",
        "        # [CLS], [SEP], [PAD]の処理\n",
        "        # 系列長\n",
        "        sequence_length = len(encoding['input_ids'])\n",
        "\n",
        "        # [CLS]に対応するダミーspan追加\n",
        "        # sequence_lengthには[sep][cls]が入っているが、spansの時点ではこれらは入っていないデータを使っているので、数合わせのために-2\n",
        "        # sequence_lenghtはspansの長さより長い\n",
        "        spans = [[-1, -1]] + spans[:sequence_length-2]\n",
        "\n",
        "        # sep, padに対する追加\n",
        "        spans = spans + [[-1, -1]] * (sequence_length - len(spans))\n",
        "\n",
        "        # print('4')\n",
        "        \n",
        "        # 手順５：必要に応じて出力の型を調整\n",
        "        if return_tensors == 'pt':\n",
        "            encoding = { k : torch.tensor([v]) for k, v in encoding.items() }\n",
        "        \n",
        "        return encoding, spans\n",
        "\n",
        "\n",
        "    '''\n",
        "    「文章」と「タグ列」と「各トークンの文章中の位置」が与えられた時に、文章中に含まれる固有表現に対応する文字列や位置を特定する関数\n",
        "    文章、ラベル列の予測値、各トークンの位置から固有表現を得る（推論時に使用）\n",
        "    '''\n",
        "    def convert_bert_output_to_entities(self, text, labels, spans):\n",
        "        # 手順１：labels, spansから特殊トークンに対応する部分を取り除く\n",
        "        # ダミーのspanは-1から始まることを利用\n",
        "        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n",
        "        spans = [span for span in spans if span[0] != -1]\n",
        "\n",
        "        # 手順２：同じラベルが連続するトークンをまとめて、固有表現を抽出\n",
        "        entities = []\n",
        "\n",
        "        for label, group \\\n",
        "            in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
        "\n",
        "            # itertools groupby https://note.nkmk.me/python-itertools-groupby/\n",
        "            group = list(group)\n",
        "            start = spans[group[0][0]][0]\n",
        "            end = spans[group[-1][0]][1]\n",
        "\n",
        "            if label != 0: # ラベルが0以外ならば、新たな固有表現として追加。\n",
        "                entity = {\n",
        "                    \"name\": text[start:end],\n",
        "                    \"span\": [start, end],\n",
        "                    \"type_id\": label\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wQepsOxT-3Ce"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 上記のトークナイザーをインスタンス化\n",
        "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4K5rK7roxVT",
        "outputId": "94f95908-c010-4032-c4b3-fa64612974c6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
            "The class this function is called from is 'NER_tokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode_plus_taggedの挙動確認\n",
        "text = '昨日のみらい事務所との打ち合わせは順調だった'\n",
        "\n",
        "# ちなみにIO法のOタグは0(zero), I タグは１以上の数字で表される \n",
        "entities = [\n",
        "    {'name' : 'みらい事務所', 'span' : [3, 9], 'type_id' : 1}\n",
        "]\n",
        "\n",
        "encoding = tokenizer.encode_plus_tagged(\n",
        "    text, entities, max_length=20\n",
        ")\n",
        "print(encoding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69OKdPyBpHJM",
        "outputId": "f78a21b3-d659-4554-9c53-f8da863083b5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [2, 10271, 28486, 5, 546, 10780, 2464, 13, 5, 1878, 2682, 9, 10750, 308, 10, 3, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], 'labels': [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode_plus_untaggedの挙動確認\n",
        "\n",
        "text = '騰訊の英語名はTencent Holdings Ltdである。'\n",
        "encoding, spans = tokenizer.encode_plus_untagged(\n",
        "    text, return_tensors='pt'\n",
        ")\n",
        "print('# encoding')\n",
        "print(encoding)\n",
        "print('# spans')\n",
        "print(spans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlxkcM3Tq8Vg",
        "outputId": "2fea92bd-90d4-47f0-d848-1b336dfc58c5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# encoding\n",
            "{'input_ids': tensor([[    2,     1, 26280,     5,  1543,   125,     9,  6749, 28550,  2953,\n",
            "         28550, 28566, 21202, 28683, 14050, 12475,    12,    31,     8,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "# spans\n",
            "[[-1, -1], [0, 1], [1, 2], [2, 3], [3, 5], [5, 6], [6, 7], [7, 9], [9, 10], [10, 12], [12, 13], [13, 14], [15, 18], [18, 19], [19, 23], [24, 27], [27, 28], [28, 30], [30, 31], [-1, -1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert_bert_output_to_entitiesの挙動確認\n",
        "# 適当なバートからの出力を置いておく\n",
        "labels_predicted = [0, 1, 1, 0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0]\n",
        "\n",
        "entities = tokenizer.convert_bert_output_to_entities(\n",
        "    text, labels_predicted, spans\n",
        ")\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ1RUfJPu25E",
        "outputId": "7c30f6f0-8add-4b07-b1fa-5f5323b7a79d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'name': '騰訊', 'span': [0, 2], 'type_id': 1}, {'name': 'Tencent Holdings Ltd', 'span': [7, 27], 'type_id': 1}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "ここからは、BERTも使って固有表現抽出を行っていく\n",
        "BertForTokenClassificationが使用できる\n",
        "'''\n",
        "\n",
        "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# 固有表現のタイプが３つ　＋　Oタグでlabel数は4\n",
        "# 1:組織名、２：人名、３：製品名\n",
        "bert_tc = BertForTokenClassification.from_pretrained(\n",
        "    MODEL_NAME, num_labels = 4\n",
        ")\n",
        "\n",
        "bert_tc = bert_tc.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pexGuFJvZeX",
        "outputId": "45a5942b-c917-4cc5-8326-4ee04b3dba63"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
            "The class this function is called from is 'NER_tokenizer'.\n",
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "推論時の使い方\n",
        "'''\n",
        "\n",
        "# データの定義\n",
        "text = 'AさんはB大学に入学した。'\n",
        "\n",
        "# 符号化を行い、トークンの文章中の位置も特定する\n",
        "encoding, spans = tokenizer.encode_plus_untagged(\n",
        "    text, return_tensors = 'pt'\n",
        ")\n",
        "\n",
        "encoding = { k : v.cuda() for k, v in encoding.items() }\n",
        "\n",
        "# BERTでトークンごとの分類スコアを出力\n",
        "# スコアが最も高いラベルを予測値とする\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = bert_tc(**encoding)\n",
        "    # scores[バッチサイズ, 系列長, ラベルの数]\n",
        "    scores = output.logits\n",
        "    labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist()\n",
        "\n",
        "# ラベルを固有表現に変換\n",
        "entities = tokenizer.convert_bert_output_to_entities(text, labels_predicted, spans)\n",
        "print(entities)\n",
        "\n",
        "\n",
        "# ちなみに、チューニングしていないので出力は出鱈目である"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvyPCsvKwPK7",
        "outputId": "75f26088-7e95-4470-a30b-94a7d437a270"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'name': 'さん', 'span': [1, 3], 'type_id': 1}, {'name': 'は', 'span': [3, 4], 'type_id': 3}, {'name': '大学', 'span': [5, 7], 'type_id': 3}, {'name': 'に', 'span': [7, 8], 'type_id': 1}, {'name': '入学', 'span': [8, 10], 'type_id': 3}, {'name': 'した。', 'span': [10, 13], 'type_id': 1}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "学習する際のコード\n",
        "\n",
        "'''\n",
        "\n",
        "# 学習用データの定義\n",
        "data = [\n",
        "    {\n",
        "        'text': 'AさんはB大学に入学した。',\n",
        "        'entities': [\n",
        "            {'name': 'A', 'span': [0, 1], 'type_id': 2},\n",
        "            {'name': 'B大学', 'span': [4, 7], 'type_id': 1}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        'text': 'CDE株式会社は新製品「E」を販売する。',\n",
        "        'entities': [\n",
        "            {'name': 'CDE株式会社', 'span': [0, 7], 'type_id': 1},\n",
        "            {'name': 'E', 'span': [12, 13], 'type_id': 3}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# 各データを符号化\n",
        "max_length = 32\n",
        "\n",
        "# 符号化結果を格納\n",
        "dataset_for_loader = []\n",
        "\n",
        "# 符号化\n",
        "for sample in data:\n",
        "    text = sample['text']\n",
        "    entities = sample['entities']\n",
        "\n",
        "    encoding = tokenizer.encode_plus_tagged(\n",
        "        text, entities, max_length\n",
        "    )\n",
        "    encoding = { k : torch.tensor(v) for k, v in encoding.items() }\n",
        "    dataset_for_loader.append(encoding)\n",
        "\n",
        "\n",
        "# データローダを作成\n",
        "dataloader = DataLoader(dataset_for_loader, batch_size = len(data))\n",
        "\n",
        "# ミニバッチを取り出して損失を得る\n",
        "for batch in dataloader:\n",
        "    batch = { k : v.cuda() for k, v in batch.items() }\n",
        "    output = bert_tc(**batch)\n",
        "    loss = output.loss\n"
      ],
      "metadata": {
        "id": "VwqKJF9V0C2c"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}